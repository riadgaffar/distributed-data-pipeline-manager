# Input section: Consumes data from Kafka
input:
  kafka:
    addresses: ["${KAFKA_BROKERS}"]
    topics: ["${KAFKA_TOPICS}"]
    consumer_group: "${KAFKA_CONSUMER_GROUP}"
    start_from_oldest: true

# Pipeline section: Processes and transforms data
pipeline:
  processors:
    - dedupe:
        cache: dedup_cache
        key: "${! content() }"
    - mapping: |
        root.id = uuid_v4()
        root.timestamp = now()
        root.data = content().uppercase()
    - switch:
        - check: "\"${LOG_LEVEL}\" == \"DEBUG\""
          processors:
            - log:
                level: "${LOG_LEVEL}"
                message: 'Inserting record into SQL: ${! json() }'
    # Count all incoming messages
    - metric:
        type: counter
        name: kafka_postgres_messages_total
        value: "1"
        labels:
          service: "redpanda"
          pipeline: "kafka_to_postgres"

    # Count dropped messages (when insert fails)
    - catch:
        - metric:
            type: counter
            name: kafka_postgres_messages_dropped
            value: "1"
            labels:
              service: "redpanda"
              pipeline: "kafka_to_postgres"

        - metric:
            type: counter
            name: pipeline_error_count
            value: "1"
            labels:
              error_type: "${! error() }"

        - metric:
            type: timing
            name: pipeline_processing_latency_seconds
            labels:
              service: "redpanda"
              pipeline: "kafka_to_postgres"
              stage: "processing"

        - log:
            level: ERROR
            message: "Error in pipeline: ${! error() }"

# Output section: Routes data to multiple destinations
output:
  fallback:
    - retry:
        max_retries: 5
        backoff:
          initial_interval: 1s
        output:
          sql_insert:
            driver: postgres
            dsn: "${POSTGRES_URL}"
            table: "${POSTGRES_TABLE}"
            columns: ["id", "timestamp", "data"]
            args_mapping: |
              root = [
                this.id,
                this.timestamp,
                this.data
              ]
            suffix: ON CONFLICT (data, timestamp) DO UPDATE SET timestamp = EXCLUDED.timestamp
            max_in_flight: 10
            batching:
              count: 100  # Number of messages per batch
              period: 100ms  # Time window for batching

    - redpanda:
        seed_brokers: ["${KAFKA_BROKERS}"]
        topic: "failed-messages"
        compression: gzip

    - stdout: {}

# Logger section: Controls logging verbosity
logger:
  level: "${LOG_LEVEL}"

# Metrics section: Exposes metrics directly via HTTP
http:
  address: "127.0.0.1:4195"

metrics:
  prometheus:
    use_histogram_timing: true

# Cache resources section: Define the cache for deduplication
cache_resources:
  - label: dedup_cache
    memory:
      default_ttl: 60s
