# Input section: Consumes data from Kafka
input:
  kafka:
    addresses: ["${KAFKA_BROKERS}"]
    topics: ["${KAFKA_TOPICS}"]
    consumer_group: "${KAFKA_CONSUMER_GROUP}"
    start_from_oldest: true

# Pipeline section: Processes and transforms data
pipeline:
  processors:
    - dedupe:
        cache: dedup_cache
        key: "${! content() }"
    - mapping: |
        root.id = uuid_v4()
        root.timestamp = now()
        root.data = content().uppercase()
    - switch:
        - check: "\"${LOG_LEVEL}\" == \"DEBUG\""
          processors:
            - log:
                level: "${LOG_LEVEL}"
                message: 'Inserting record into SQL: ${! json() }'

# Output section: Routes data to multiple destinations
output:
  fallback:
    - retry:
        max_retries: 3
        backoff:
          initial_interval: 500ms
        output:
          sql_insert:
            driver: postgres
            dsn: "${POSTGRES_URL}"
            table: "${POSTGRES_TABLE}"
            columns: ["id", "timestamp", "data"]
            args_mapping: |
              root = [
                this.id,
                this.timestamp,
                this.data
              ]
            suffix: ON CONFLICT (data, timestamp) DO UPDATE SET timestamp = EXCLUDED.timestamp
            max_in_flight: 1

    - redpanda:
        seed_brokers: ["${KAFKA_BROKERS}"]
        topic: "failed-messages"
        compression: gzip

    - stdout: {}

# Logger section: Controls logging verbosity
logger:
  level: "${LOG_LEVEL}"

# Cache resources section: Define the cache for deduplication
cache_resources:
  - label: dedup_cache
    memory:
      default_ttl: 60s
